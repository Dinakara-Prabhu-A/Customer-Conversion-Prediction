{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: F1 Score = 0.5393\n",
      "Naive Bayes: F1 Score = 0.4478\n",
      "Support Vector Classifier: F1 Score = 0.5641\n",
      "K-Nearest Neighbors: F1 Score = 0.5101\n",
      "Decision Tree: F1 Score = 0.4690\n",
      "Random Forest: F1 Score = 0.5576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Dinakaraprabhu/Data Science Projects/Customer_Conversion_Prediction/venv/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost: F1 Score = 0.5443\n",
      "Gradient Boosting: F1 Score = 0.5810\n",
      "XGBoost: F1 Score = 0.5671\n",
      "\n",
      "------Final F1 Scores for All Classifiers:-------\n",
      "Logistic Regression: 0.5393\n",
      "Naive Bayes: 0.4478\n",
      "Support Vector Classifier: 0.5641\n",
      "K-Nearest Neighbors: 0.5101\n",
      "Decision Tree: 0.4690\n",
      "Random Forest: 0.5576\n",
      "AdaBoost: 0.5443\n",
      "Gradient Boosting: 0.5810\n",
      "XGBoost: 0.5671\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# import necessary modules\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier,AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "# prepare column transformers\n",
    "\n",
    "def create_column_transformer(numeric_features, categorical_features):\n",
    "    numeric_transformer = StandardScaler()\n",
    "\n",
    "    categorical_transformer = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ],\n",
    "        remainder='passthrough'  # To pass through any features not specified in the transformers\n",
    "    )\n",
    "    return preprocessor\n",
    "\n",
    "\n",
    "# function to evaluate the models\n",
    "\n",
    "def evaluate_classifiers(X, y, numeric_features, categorical_features):\n",
    "    preprocessor = create_column_transformer(numeric_features, categorical_features)\n",
    "    \n",
    "    classifiers = {\n",
    "        'Logistic Regression': LogisticRegression(),\n",
    "        'Naive Bayes': GaussianNB(),\n",
    "        'Support Vector Classifier': SVC(),\n",
    "        'K-Nearest Neighbors': KNeighborsClassifier(),\n",
    "        'Decision Tree': DecisionTreeClassifier(),\n",
    "        'Random Forest': RandomForestClassifier(),\n",
    "        'AdaBoost': AdaBoostClassifier(),  \n",
    "        'Gradient Boosting': GradientBoostingClassifier(),\n",
    "        'XGBoost': XGBClassifier(),\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, classifier in classifiers.items():\n",
    "        pipeline = ImbPipeline(steps=[\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('smote', SMOTE()),  # Handle class imbalance using SMOTE\n",
    "            ('classifier', classifier)\n",
    "        ])\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "        \n",
    "        pipeline.fit(X_train, y_train)\n",
    "        y_pred = pipeline.predict(X_test)\n",
    "        \n",
    "        f1 = f1_score(y_test, y_pred, average='binary')\n",
    "        results[name] = f1\n",
    "        print(f'{name}: F1 Score = {f1:.4f}')\n",
    "    \n",
    "    return results\n",
    "\n",
    "# main:\n",
    "# load the datframe from the csv file\n",
    "\n",
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "\n",
    "\n",
    "X = df.drop(columns=['y'])\n",
    "y = df['y']\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "y =pd.Series(y)\n",
    "# Specify which columns are numeric and which are categorical\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Evaluate all classifiers\n",
    "results = evaluate_classifiers(X, y, numeric_features, categorical_features)\n",
    "\n",
    "# Display results\n",
    "print(\"\\n------Final F1 Scores for All Classifiers:-------\")\n",
    "for model_name, f1 in results.items():\n",
    "    print(f'{model_name}: {f1:.4f}')\n",
    "print(\"-------\"*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "[CV] END classifier__learning_rate=0.1, classifier__max_depth=4, classifier__min_samples_leaf=1, classifier__min_samples_split=5, classifier__n_estimators=100, classifier__subsample=1.0; total time=  34.5s\n",
      "[CV] END classifier__learning_rate=0.1, classifier__max_depth=4, classifier__min_samples_leaf=1, classifier__min_samples_split=5, classifier__n_estimators=100, classifier__subsample=1.0; total time=  34.9s\n",
      "[CV] END classifier__learning_rate=0.1, classifier__max_depth=4, classifier__min_samples_leaf=1, classifier__min_samples_split=10, classifier__n_estimators=200, classifier__subsample=0.8; total time=  54.9s\n",
      "[CV] END classifier__learning_rate=0.1, classifier__max_depth=4, classifier__min_samples_leaf=1, classifier__min_samples_split=10, classifier__n_estimators=200, classifier__subsample=0.8; total time=  55.3s\n",
      "[CV] END classifier__learning_rate=0.1, classifier__max_depth=4, classifier__min_samples_leaf=1, classifier__min_samples_split=10, classifier__n_estimators=200, classifier__subsample=0.8; total time=  55.3s\n",
      "[CV] END classifier__learning_rate=0.1, classifier__max_depth=4, classifier__min_samples_leaf=1, classifier__min_samples_split=5, classifier__n_estimators=100, classifier__subsample=1.0; total time=  33.6s\n",
      "[CV] END classifier__learning_rate=0.1, classifier__max_depth=4, classifier__min_samples_leaf=1, classifier__min_samples_split=5, classifier__n_estimators=300, classifier__subsample=1.0; total time= 1.8min\n",
      "[CV] END classifier__learning_rate=0.1, classifier__max_depth=4, classifier__min_samples_leaf=1, classifier__min_samples_split=5, classifier__n_estimators=300, classifier__subsample=1.0; total time= 1.8min\n",
      "[CV] END classifier__learning_rate=0.1, classifier__max_depth=4, classifier__min_samples_leaf=1, classifier__min_samples_split=5, classifier__n_estimators=300, classifier__subsample=1.0; total time= 1.8min\n",
      "[CV] END classifier__learning_rate=0.1, classifier__max_depth=4, classifier__min_samples_leaf=2, classifier__min_samples_split=10, classifier__n_estimators=300, classifier__subsample=0.8; total time= 1.5min\n",
      "[CV] END classifier__learning_rate=0.05, classifier__max_depth=4, classifier__min_samples_leaf=1, classifier__min_samples_split=10, classifier__n_estimators=200, classifier__subsample=1.0; total time= 1.3min\n",
      "[CV] END classifier__learning_rate=0.05, classifier__max_depth=4, classifier__min_samples_leaf=1, classifier__min_samples_split=10, classifier__n_estimators=200, classifier__subsample=1.0; total time= 1.3min\n",
      "[CV] END classifier__learning_rate=0.1, classifier__max_depth=4, classifier__min_samples_leaf=1, classifier__min_samples_split=10, classifier__n_estimators=100, classifier__subsample=1.0; total time=  37.8s\n",
      "[CV] END classifier__learning_rate=0.1, classifier__max_depth=4, classifier__min_samples_leaf=1, classifier__min_samples_split=10, classifier__n_estimators=100, classifier__subsample=1.0; total time=  37.5s\n",
      "[CV] END classifier__learning_rate=0.1, classifier__max_depth=4, classifier__min_samples_leaf=2, classifier__min_samples_split=10, classifier__n_estimators=300, classifier__subsample=0.8; total time= 1.5min\n",
      "[CV] END classifier__learning_rate=0.1, classifier__max_depth=4, classifier__min_samples_leaf=2, classifier__min_samples_split=10, classifier__n_estimators=300, classifier__subsample=0.8; total time= 1.5min\n",
      "[CV] END classifier__learning_rate=0.1, classifier__max_depth=4, classifier__min_samples_leaf=1, classifier__min_samples_split=10, classifier__n_estimators=100, classifier__subsample=1.0; total time=  37.5s\n",
      "[CV] END classifier__learning_rate=0.05, classifier__max_depth=4, classifier__min_samples_leaf=2, classifier__min_samples_split=10, classifier__n_estimators=100, classifier__subsample=0.8; total time=  28.9s\n",
      "[CV] END classifier__learning_rate=0.05, classifier__max_depth=4, classifier__min_samples_leaf=2, classifier__min_samples_split=10, classifier__n_estimators=100, classifier__subsample=0.8; total time=  28.9s\n",
      "[CV] END classifier__learning_rate=0.05, classifier__max_depth=4, classifier__min_samples_leaf=2, classifier__min_samples_split=10, classifier__n_estimators=100, classifier__subsample=0.8; total time=  28.7s\n",
      "[CV] END classifier__learning_rate=0.05, classifier__max_depth=4, classifier__min_samples_leaf=1, classifier__min_samples_split=10, classifier__n_estimators=200, classifier__subsample=1.0; total time= 1.2min\n",
      "[CV] END classifier__learning_rate=0.05, classifier__max_depth=3, classifier__min_samples_leaf=1, classifier__min_samples_split=5, classifier__n_estimators=100, classifier__subsample=0.8; total time=  21.6s\n",
      "[CV] END classifier__learning_rate=0.1, classifier__max_depth=3, classifier__min_samples_leaf=2, classifier__min_samples_split=10, classifier__n_estimators=200, classifier__subsample=1.0; total time=  54.9s\n",
      "[CV] END classifier__learning_rate=0.05, classifier__max_depth=3, classifier__min_samples_leaf=1, classifier__min_samples_split=5, classifier__n_estimators=100, classifier__subsample=0.8; total time=  22.4s\n",
      "[CV] END classifier__learning_rate=0.05, classifier__max_depth=3, classifier__min_samples_leaf=1, classifier__min_samples_split=5, classifier__n_estimators=100, classifier__subsample=0.8; total time=  22.3s\n",
      "[CV] END classifier__learning_rate=0.1, classifier__max_depth=3, classifier__min_samples_leaf=2, classifier__min_samples_split=10, classifier__n_estimators=200, classifier__subsample=1.0; total time=  54.5s\n",
      "[CV] END classifier__learning_rate=0.1, classifier__max_depth=3, classifier__min_samples_leaf=2, classifier__min_samples_split=10, classifier__n_estimators=200, classifier__subsample=1.0; total time=  54.8s\n",
      "[CV] END classifier__learning_rate=0.05, classifier__max_depth=4, classifier__min_samples_leaf=1, classifier__min_samples_split=10, classifier__n_estimators=100, classifier__subsample=0.8; total time=  26.8s\n",
      "[CV] END classifier__learning_rate=0.05, classifier__max_depth=4, classifier__min_samples_leaf=1, classifier__min_samples_split=10, classifier__n_estimators=100, classifier__subsample=0.8; total time=  26.7s\n",
      "[CV] END classifier__learning_rate=0.05, classifier__max_depth=3, classifier__min_samples_leaf=1, classifier__min_samples_split=10, classifier__n_estimators=300, classifier__subsample=0.8; total time= 1.1min\n",
      "[CV] END classifier__learning_rate=0.05, classifier__max_depth=3, classifier__min_samples_leaf=1, classifier__min_samples_split=10, classifier__n_estimators=300, classifier__subsample=0.8; total time= 1.1min\n",
      "[CV] END classifier__learning_rate=0.05, classifier__max_depth=3, classifier__min_samples_leaf=2, classifier__min_samples_split=10, classifier__n_estimators=100, classifier__subsample=0.8; total time=  22.5s\n",
      "[CV] END classifier__learning_rate=0.05, classifier__max_depth=3, classifier__min_samples_leaf=1, classifier__min_samples_split=10, classifier__n_estimators=300, classifier__subsample=0.8; total time= 1.1min\n",
      "[CV] END classifier__learning_rate=0.1, classifier__max_depth=3, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=300, classifier__subsample=0.8; total time= 1.1min\n",
      "[CV] END classifier__learning_rate=0.05, classifier__max_depth=4, classifier__min_samples_leaf=1, classifier__min_samples_split=10, classifier__n_estimators=100, classifier__subsample=0.8; total time=  28.3s\n",
      "[CV] END classifier__learning_rate=0.1, classifier__max_depth=3, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=300, classifier__subsample=0.8; total time= 1.1min\n",
      "[CV] END classifier__learning_rate=0.1, classifier__max_depth=3, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=300, classifier__subsample=0.8; total time= 1.1min\n",
      "[CV] END classifier__learning_rate=0.05, classifier__max_depth=3, classifier__min_samples_leaf=2, classifier__min_samples_split=10, classifier__n_estimators=100, classifier__subsample=0.8; total time=  20.5s\n",
      "[CV] END classifier__learning_rate=0.05, classifier__max_depth=3, classifier__min_samples_leaf=2, classifier__min_samples_split=10, classifier__n_estimators=100, classifier__subsample=0.8; total time=  21.2s\n",
      "[CV] END classifier__learning_rate=0.05, classifier__max_depth=4, classifier__min_samples_leaf=1, classifier__min_samples_split=10, classifier__n_estimators=100, classifier__subsample=1.0; total time=  31.2s\n",
      "[CV] END classifier__learning_rate=0.05, classifier__max_depth=4, classifier__min_samples_leaf=1, classifier__min_samples_split=10, classifier__n_estimators=100, classifier__subsample=1.0; total time=  32.0s\n",
      "[CV] END classifier__learning_rate=0.05, classifier__max_depth=4, classifier__min_samples_leaf=1, classifier__min_samples_split=10, classifier__n_estimators=100, classifier__subsample=1.0; total time=  31.6s\n",
      "[CV] END classifier__learning_rate=0.05, classifier__max_depth=3, classifier__min_samples_leaf=1, classifier__min_samples_split=5, classifier__n_estimators=300, classifier__subsample=0.8; total time= 1.0min\n",
      "[CV] END classifier__learning_rate=0.05, classifier__max_depth=3, classifier__min_samples_leaf=1, classifier__min_samples_split=5, classifier__n_estimators=300, classifier__subsample=0.8; total time= 1.0min\n",
      "[CV] END classifier__learning_rate=0.05, classifier__max_depth=3, classifier__min_samples_leaf=1, classifier__min_samples_split=5, classifier__n_estimators=300, classifier__subsample=0.8; total time= 1.0min\n",
      "[CV] END classifier__learning_rate=0.05, classifier__max_depth=3, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=100, classifier__subsample=0.8; total time=  20.9s\n",
      "[CV] END classifier__learning_rate=0.05, classifier__max_depth=3, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=100, classifier__subsample=0.8; total time=  21.3s\n",
      "[CV] END classifier__learning_rate=0.05, classifier__max_depth=4, classifier__min_samples_leaf=1, classifier__min_samples_split=5, classifier__n_estimators=200, classifier__subsample=0.8; total time=  52.7s\n",
      "[CV] END classifier__learning_rate=0.05, classifier__max_depth=4, classifier__min_samples_leaf=1, classifier__min_samples_split=5, classifier__n_estimators=200, classifier__subsample=0.8; total time=  53.0s\n",
      "[CV] END classifier__learning_rate=0.05, classifier__max_depth=3, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=100, classifier__subsample=0.8; total time=  19.7s\n",
      "[CV] END classifier__learning_rate=0.05, classifier__max_depth=4, classifier__min_samples_leaf=1, classifier__min_samples_split=5, classifier__n_estimators=200, classifier__subsample=0.8; total time=  51.5s\n",
      "[CV] END classifier__learning_rate=0.1, classifier__max_depth=4, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=100, classifier__subsample=1.0; total time=  32.2s\n",
      "[CV] END classifier__learning_rate=0.1, classifier__max_depth=4, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=100, classifier__subsample=1.0; total time=  32.1s\n",
      "[CV] END classifier__learning_rate=0.05, classifier__max_depth=3, classifier__min_samples_leaf=2, classifier__min_samples_split=10, classifier__n_estimators=300, classifier__subsample=0.8; total time=  59.7s\n",
      "[CV] END classifier__learning_rate=0.05, classifier__max_depth=3, classifier__min_samples_leaf=2, classifier__min_samples_split=10, classifier__n_estimators=300, classifier__subsample=0.8; total time=  58.4s\n",
      "[CV] END classifier__learning_rate=0.05, classifier__max_depth=3, classifier__min_samples_leaf=2, classifier__min_samples_split=10, classifier__n_estimators=300, classifier__subsample=0.8; total time=  58.1s\n",
      "[CV] END classifier__learning_rate=0.1, classifier__max_depth=3, classifier__min_samples_leaf=1, classifier__min_samples_split=5, classifier__n_estimators=300, classifier__subsample=1.0; total time= 1.1min\n",
      "[CV] END classifier__learning_rate=0.1, classifier__max_depth=3, classifier__min_samples_leaf=1, classifier__min_samples_split=5, classifier__n_estimators=300, classifier__subsample=1.0; total time= 1.1min\n",
      "[CV] END classifier__learning_rate=0.1, classifier__max_depth=3, classifier__min_samples_leaf=1, classifier__min_samples_split=5, classifier__n_estimators=300, classifier__subsample=1.0; total time= 1.0min\n",
      "[CV] END classifier__learning_rate=0.1, classifier__max_depth=4, classifier__min_samples_leaf=2, classifier__min_samples_split=5, classifier__n_estimators=100, classifier__subsample=1.0; total time=  21.1s\n",
      "Best F1 Score for Gradient Boosting: 0.5968\n",
      "Best Parameters for Gradient Boosting: {'classifier__subsample': 1.0, 'classifier__n_estimators': 200, 'classifier__min_samples_split': 10, 'classifier__min_samples_leaf': 2, 'classifier__max_depth': 3, 'classifier__learning_rate': 0.1}\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "[CV] END classifier__colsample_bytree=1.0, classifier__gamma=0.1, classifier__learning_rate=0.1, classifier__max_depth=3, classifier__min_child_weight=3, classifier__n_estimators=100; total time=   1.9s\n",
      "[CV] END classifier__colsample_bytree=1.0, classifier__gamma=0.1, classifier__learning_rate=0.1, classifier__max_depth=3, classifier__min_child_weight=3, classifier__n_estimators=100; total time=   1.8s\n",
      "[CV] END classifier__colsample_bytree=1.0, classifier__gamma=0.1, classifier__learning_rate=0.1, classifier__max_depth=3, classifier__min_child_weight=3, classifier__n_estimators=100; total time=   2.2s\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__gamma=0, classifier__learning_rate=0.05, classifier__max_depth=3, classifier__min_child_weight=1, classifier__n_estimators=100; total time=   2.6s\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__gamma=0, classifier__learning_rate=0.05, classifier__max_depth=3, classifier__min_child_weight=1, classifier__n_estimators=100; total time=   2.6s\n",
      "[CV] END classifier__colsample_bytree=1.0, classifier__gamma=0.1, classifier__learning_rate=0.05, classifier__max_depth=4, classifier__min_child_weight=1, classifier__n_estimators=100; total time=   3.1s\n",
      "[CV] END classifier__colsample_bytree=1.0, classifier__gamma=0.1, classifier__learning_rate=0.05, classifier__max_depth=4, classifier__min_child_weight=1, classifier__n_estimators=100; total time=   3.4s\n",
      "[CV] END classifier__colsample_bytree=1.0, classifier__gamma=0.1, classifier__learning_rate=0.05, classifier__max_depth=4, classifier__min_child_weight=1, classifier__n_estimators=100; total time=   3.4s\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__gamma=0, classifier__learning_rate=0.05, classifier__max_depth=3, classifier__min_child_weight=1, classifier__n_estimators=100; total time=   3.5s\n",
      "[CV] END classifier__colsample_bytree=1.0, classifier__gamma=0, classifier__learning_rate=0.1, classifier__max_depth=4, classifier__min_child_weight=1, classifier__n_estimators=100; total time=   3.8s\n",
      "[CV] END classifier__colsample_bytree=1.0, classifier__gamma=0, classifier__learning_rate=0.1, classifier__max_depth=4, classifier__min_child_weight=1, classifier__n_estimators=100; total time=   3.7s\n",
      "[CV] END classifier__colsample_bytree=1.0, classifier__gamma=0, classifier__learning_rate=0.1, classifier__max_depth=4, classifier__min_child_weight=1, classifier__n_estimators=100; total time=   3.5s\n",
      "[CV] END classifier__colsample_bytree=1.0, classifier__gamma=0, classifier__learning_rate=0.05, classifier__max_depth=4, classifier__min_child_weight=1, classifier__n_estimators=100; total time=   3.5s\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__gamma=0, classifier__learning_rate=0.05, classifier__max_depth=4, classifier__min_child_weight=1, classifier__n_estimators=200; total time=   5.4s\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__gamma=0, classifier__learning_rate=0.05, classifier__max_depth=4, classifier__min_child_weight=1, classifier__n_estimators=200; total time=   5.4s\n",
      "[CV] END classifier__colsample_bytree=1.0, classifier__gamma=0, classifier__learning_rate=0.05, classifier__max_depth=4, classifier__min_child_weight=1, classifier__n_estimators=100; total time=   3.1s\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__gamma=0, classifier__learning_rate=0.05, classifier__max_depth=4, classifier__min_child_weight=1, classifier__n_estimators=200; total time=   5.2s\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__gamma=0.1, classifier__learning_rate=0.05, classifier__max_depth=3, classifier__min_child_weight=1, classifier__n_estimators=100; total time=   2.3s\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__gamma=0.1, classifier__learning_rate=0.05, classifier__max_depth=3, classifier__min_child_weight=1, classifier__n_estimators=100; total time=   2.4s\n",
      "[CV] END classifier__colsample_bytree=1.0, classifier__gamma=0, classifier__learning_rate=0.05, classifier__max_depth=4, classifier__min_child_weight=1, classifier__n_estimators=100; total time=   2.9s\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__gamma=0.1, classifier__learning_rate=0.05, classifier__max_depth=3, classifier__min_child_weight=1, classifier__n_estimators=100; total time=   2.3s\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__gamma=0, classifier__learning_rate=0.1, classifier__max_depth=4, classifier__min_child_weight=1, classifier__n_estimators=100; total time=   2.4s\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__gamma=0, classifier__learning_rate=0.1, classifier__max_depth=4, classifier__min_child_weight=1, classifier__n_estimators=100; total time=   2.2s\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__gamma=0, classifier__learning_rate=0.1, classifier__max_depth=4, classifier__min_child_weight=1, classifier__n_estimators=100; total time=   2.3s\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__gamma=0.1, classifier__learning_rate=0.1, classifier__max_depth=3, classifier__min_child_weight=1, classifier__n_estimators=200; total time=   3.3s\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__gamma=0.1, classifier__learning_rate=0.1, classifier__max_depth=3, classifier__min_child_weight=1, classifier__n_estimators=200; total time=   3.5s\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__gamma=0.1, classifier__learning_rate=0.1, classifier__max_depth=3, classifier__min_child_weight=1, classifier__n_estimators=200; total time=   3.6s\n",
      "[CV] END classifier__colsample_bytree=1.0, classifier__gamma=0.1, classifier__learning_rate=0.1, classifier__max_depth=4, classifier__min_child_weight=1, classifier__n_estimators=200; total time=   4.3s\n",
      "[CV] END classifier__colsample_bytree=1.0, classifier__gamma=0.1, classifier__learning_rate=0.1, classifier__max_depth=3, classifier__min_child_weight=1, classifier__n_estimators=100; total time=   2.3s\n",
      "[CV] END classifier__colsample_bytree=1.0, classifier__gamma=0.1, classifier__learning_rate=0.1, classifier__max_depth=3, classifier__min_child_weight=1, classifier__n_estimators=100; total time=   2.4s\n",
      "[CV] END classifier__colsample_bytree=1.0, classifier__gamma=0.1, classifier__learning_rate=0.1, classifier__max_depth=4, classifier__min_child_weight=1, classifier__n_estimators=200; total time=   4.3s\n",
      "[CV] END classifier__colsample_bytree=1.0, classifier__gamma=0.1, classifier__learning_rate=0.1, classifier__max_depth=3, classifier__min_child_weight=1, classifier__n_estimators=100; total time=   2.0s\n",
      "[CV] END classifier__colsample_bytree=1.0, classifier__gamma=0.1, classifier__learning_rate=0.1, classifier__max_depth=4, classifier__min_child_weight=1, classifier__n_estimators=200; total time=   4.4s\n",
      "[CV] END classifier__colsample_bytree=1.0, classifier__gamma=0, classifier__learning_rate=0.1, classifier__max_depth=3, classifier__min_child_weight=1, classifier__n_estimators=100; total time=   2.0s\n",
      "[CV] END classifier__colsample_bytree=1.0, classifier__gamma=0, classifier__learning_rate=0.1, classifier__max_depth=3, classifier__min_child_weight=1, classifier__n_estimators=100; total time=   2.1s\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__gamma=0, classifier__learning_rate=0.1, classifier__max_depth=3, classifier__min_child_weight=1, classifier__n_estimators=200; total time=   3.0s\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__gamma=0, classifier__learning_rate=0.1, classifier__max_depth=3, classifier__min_child_weight=1, classifier__n_estimators=200; total time=   3.3s\n",
      "[CV] END classifier__colsample_bytree=1.0, classifier__gamma=0, classifier__learning_rate=0.1, classifier__max_depth=3, classifier__min_child_weight=1, classifier__n_estimators=100; total time=   2.2s\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__gamma=0, classifier__learning_rate=0.1, classifier__max_depth=3, classifier__min_child_weight=1, classifier__n_estimators=200; total time=   3.3s\n",
      "[CV] END classifier__colsample_bytree=1.0, classifier__gamma=0, classifier__learning_rate=0.05, classifier__max_depth=3, classifier__min_child_weight=3, classifier__n_estimators=100; total time=   1.9s\n",
      "[CV] END classifier__colsample_bytree=1.0, classifier__gamma=0, classifier__learning_rate=0.05, classifier__max_depth=3, classifier__min_child_weight=3, classifier__n_estimators=100; total time=   2.0s\n",
      "[CV] END classifier__colsample_bytree=1.0, classifier__gamma=0, classifier__learning_rate=0.05, classifier__max_depth=3, classifier__min_child_weight=3, classifier__n_estimators=100; total time=   2.0s\n",
      "[CV] END classifier__colsample_bytree=1.0, classifier__gamma=0.1, classifier__learning_rate=0.05, classifier__max_depth=3, classifier__min_child_weight=3, classifier__n_estimators=100; total time=   2.0s\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__gamma=0, classifier__learning_rate=0.1, classifier__max_depth=4, classifier__min_child_weight=1, classifier__n_estimators=200; total time=   3.8s\n",
      "[CV] END classifier__colsample_bytree=1.0, classifier__gamma=0.1, classifier__learning_rate=0.05, classifier__max_depth=3, classifier__min_child_weight=3, classifier__n_estimators=100; total time=   2.1s\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__gamma=0, classifier__learning_rate=0.1, classifier__max_depth=4, classifier__min_child_weight=1, classifier__n_estimators=200; total time=   4.2s\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__gamma=0, classifier__learning_rate=0.1, classifier__max_depth=4, classifier__min_child_weight=1, classifier__n_estimators=200; total time=   3.9s\n",
      "[CV] END classifier__colsample_bytree=1.0, classifier__gamma=0.1, classifier__learning_rate=0.05, classifier__max_depth=3, classifier__min_child_weight=3, classifier__n_estimators=100; total time=   2.2s\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__gamma=0, classifier__learning_rate=0.05, classifier__max_depth=3, classifier__min_child_weight=3, classifier__n_estimators=200; total time=   3.6s\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__gamma=0, classifier__learning_rate=0.05, classifier__max_depth=3, classifier__min_child_weight=3, classifier__n_estimators=200; total time=   3.6s\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__gamma=0, classifier__learning_rate=0.05, classifier__max_depth=3, classifier__min_child_weight=3, classifier__n_estimators=200; total time=   3.8s\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__gamma=0.1, classifier__learning_rate=0.05, classifier__max_depth=3, classifier__min_child_weight=1, classifier__n_estimators=200; total time=   3.6s\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__gamma=0.1, classifier__learning_rate=0.05, classifier__max_depth=3, classifier__min_child_weight=1, classifier__n_estimators=200; total time=   3.5s\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__gamma=0.1, classifier__learning_rate=0.05, classifier__max_depth=3, classifier__min_child_weight=1, classifier__n_estimators=200; total time=   3.5s\n",
      "[CV] END classifier__colsample_bytree=1.0, classifier__gamma=0.1, classifier__learning_rate=0.05, classifier__max_depth=3, classifier__min_child_weight=1, classifier__n_estimators=200; total time=   3.5s\n",
      "[CV] END classifier__colsample_bytree=1.0, classifier__gamma=0.1, classifier__learning_rate=0.05, classifier__max_depth=3, classifier__min_child_weight=1, classifier__n_estimators=200; total time=   3.4s\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__gamma=0, classifier__learning_rate=0.1, classifier__max_depth=3, classifier__min_child_weight=1, classifier__n_estimators=100; total time=   1.7s\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__gamma=0, classifier__learning_rate=0.1, classifier__max_depth=3, classifier__min_child_weight=1, classifier__n_estimators=100; total time=   1.4s\n",
      "[CV] END classifier__colsample_bytree=0.8, classifier__gamma=0, classifier__learning_rate=0.1, classifier__max_depth=3, classifier__min_child_weight=1, classifier__n_estimators=100; total time=   1.6s\n",
      "[CV] END classifier__colsample_bytree=1.0, classifier__gamma=0.1, classifier__learning_rate=0.05, classifier__max_depth=3, classifier__min_child_weight=1, classifier__n_estimators=200; total time=   2.4s\n",
      "Best F1 Score for XGBoost: 0.5948\n",
      "Best Parameters for XGBoost: {'classifier__n_estimators': 200, 'classifier__min_child_weight': 1, 'classifier__max_depth': 4, 'classifier__learning_rate': 0.1, 'classifier__gamma': 0.1, 'classifier__colsample_bytree': 1.0}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "# Step 1: Define the ColumnTransformer\n",
    "def create_column_transformer(numeric_features, categorical_features):\n",
    "    numeric_transformer = StandardScaler()\n",
    "    categorical_transformer = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ],\n",
    "        remainder='passthrough'  # To pass through any features not specified in the transformers\n",
    "    )\n",
    "    return preprocessor\n",
    "\n",
    "# Step 2: Define Optimized Hyperparameter Grids\n",
    "param_grid_gb = {\n",
    "    'classifier__n_estimators': [100, 200, 300],\n",
    "    'classifier__learning_rate': [0.05, 0.1],\n",
    "    'classifier__max_depth': [3, 4],\n",
    "    'classifier__min_samples_split': [5, 10],\n",
    "    'classifier__min_samples_leaf': [1, 2],\n",
    "    'classifier__subsample': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "param_grid_xgb = {\n",
    "    'classifier__n_estimators': [100, 200],\n",
    "    'classifier__learning_rate': [0.05, 0.1],\n",
    "    'classifier__max_depth': [3, 4],\n",
    "    'classifier__min_child_weight': [1, 3],\n",
    "    'classifier__gamma': [0, 0.1],\n",
    "    'classifier__colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Step 3: Function to Tune Hyperparameters\n",
    "def tune_hyperparameters(X, y, numeric_features, categorical_features, param_grid, model, model_name):\n",
    "    preprocessor = create_column_transformer(numeric_features, categorical_features)\n",
    "    \n",
    "    pipeline = ImbPipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('smote', SMOTE()),  # Handle class imbalance using SMOTE\n",
    "        ('classifier', model)\n",
    "    ])\n",
    "    \n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_distributions=param_grid,\n",
    "        n_iter=20,  # Reduced number of iterations\n",
    "        scoring=make_scorer(f1_score),\n",
    "        cv=3,  # Reduced number of cross-validation folds\n",
    "        verbose=2,\n",
    "        random_state=42,\n",
    "        n_jobs=-1  # Utilize all available CPU cores\n",
    "    )\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "    \n",
    "    random_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_params = random_search.best_params_  # Corrected: best_params_ (underscore)\n",
    "    best_model = random_search.best_estimator_\n",
    "    \n",
    "    y_pred = best_model.predict(X_test)\n",
    "    f1 = f1_score(y_test, y_pred, average='binary')\n",
    "    \n",
    "    print(f'Best F1 Score for {model_name}: {f1:.4f}')\n",
    "    print(f'Best Parameters for {model_name}: {best_params}')\n",
    "    \n",
    "    return best_params, f1\n",
    "\n",
    "\n",
    "# Step 4: Prepare the Data and Perform Hyperparameter Tuning\n",
    "# main:\n",
    "# load the datframe from the csv file\n",
    "\n",
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "\n",
    "\n",
    "X = df.drop(columns=['y'])\n",
    "y = df['y']\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "y =pd.Series(y)\n",
    "\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Tune Gradient Boosting\n",
    "best_params_gb, best_f1_gb = tune_hyperparameters(X, y, numeric_features, categorical_features, param_grid_gb, GradientBoostingClassifier(), \"Gradient Boosting\")\n",
    "\n",
    "# Tune XGBoost\n",
    "best_params_xgb, best_f1_xgb = tune_hyperparameters(X, y, numeric_features, categorical_features, param_grid_xgb, XGBClassifier(), \"XGBoost\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'classifier__subsample': 1.0,\n",
       "  'classifier__n_estimators': 200,\n",
       "  'classifier__min_samples_split': 10,\n",
       "  'classifier__min_samples_leaf': 2,\n",
       "  'classifier__max_depth': 3,\n",
       "  'classifier__learning_rate': 0.1},\n",
       " np.float64(0.5968271334792122))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params_gb, best_f1_gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'classifier__n_estimators': 200,\n",
       "  'classifier__min_child_weight': 1,\n",
       "  'classifier__max_depth': 4,\n",
       "  'classifier__learning_rate': 0.1,\n",
       "  'classifier__gamma': 0.1,\n",
       "  'classifier__colsample_bytree': 1.0},\n",
       " np.float64(0.5947786606129398))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params_xgb, best_f1_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost F1 Score: 0.6016\n",
      "Gradient Boosting F1 Score: 0.5990\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "# Step 1: Define the ColumnTransformer\n",
    "def create_column_transformer(numeric_features, categorical_features):\n",
    "    numeric_transformer = StandardScaler()\n",
    "    categorical_transformer = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ],\n",
    "        remainder='passthrough'  # To pass through any features not specified in the transformers\n",
    "    )\n",
    "    return preprocessor\n",
    "\n",
    "# Step 2: Define the best hyperparameters for the models\n",
    "best_params_xgb = {\n",
    "    'classifier__n_estimators': 200,\n",
    "    'classifier__min_child_weight': 1,\n",
    "    'classifier__max_depth': 4,\n",
    "    'classifier__learning_rate': 0.1,\n",
    "    'classifier__gamma': 0.1,\n",
    "    'classifier__colsample_bytree': 1.0\n",
    "}\n",
    "\n",
    "best_params_gb = {\n",
    "    'classifier__subsample': 1.0,\n",
    "    'classifier__n_estimators': 200,\n",
    "    'classifier__min_samples_split': 10,\n",
    "    'classifier__min_samples_leaf': 2,\n",
    "    'classifier__max_depth': 3,\n",
    "    'classifier__learning_rate': 0.1\n",
    "}\n",
    "\n",
    "# Step 3: Prepare the data\n",
    "# main:\n",
    "# load the datframe from the csv file\n",
    "\n",
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "\n",
    "\n",
    "X = df.drop(columns=['y'])\n",
    "y = df['y']\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "y =pd.Series(y)\n",
    "\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# Step 4: Train and save the XGBoost model\n",
    "preprocessor = create_column_transformer(numeric_features, categorical_features)\n",
    "\n",
    "xgb_pipeline = ImbPipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('smote', SMOTE()),  # Handle class imbalance using SMOTE\n",
    "    ('classifier', XGBClassifier(**{k.split('__')[1]: v for k, v in best_params_xgb.items()}))\n",
    "])\n",
    "\n",
    "xgb_pipeline.fit(X_train, y_train)\n",
    "xgb_f1 = f1_score(y_test, xgb_pipeline.predict(X_test))\n",
    "print(f\"XGBoost F1 Score: {xgb_f1:.4f}\")\n",
    "\n",
    "# Save the XGBoost model\n",
    "joblib.dump(xgb_pipeline, 'xgb_model.pkl')\n",
    "\n",
    "# Step 5: Train and save the Gradient Boosting model\n",
    "gb_pipeline = ImbPipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('smote', SMOTE()),  # Handle class imbalance using SMOTE\n",
    "    ('classifier', GradientBoostingClassifier(**{k.split('__')[1]: v for k, v in best_params_gb.items()}))\n",
    "])\n",
    "\n",
    "gb_pipeline.fit(X_train, y_train)\n",
    "gb_f1 = f1_score(y_test, gb_pipeline.predict(X_test))\n",
    "print(f\"Gradient Boosting F1 Score: {gb_f1:.4f}\")\n",
    "\n",
    "# Save the Gradient Boosting model\n",
    "# joblib.dump(gb_pipeline, 'gb_model.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education_qual</th>\n",
       "      <th>call_type</th>\n",
       "      <th>day</th>\n",
       "      <th>mon</th>\n",
       "      <th>dur</th>\n",
       "      <th>num_calls</th>\n",
       "      <th>prev_outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>37735</th>\n",
       "      <td>40</td>\n",
       "      <td>blue-collar</td>\n",
       "      <td>divorced</td>\n",
       "      <td>secondary</td>\n",
       "      <td>cellular</td>\n",
       "      <td>14</td>\n",
       "      <td>may</td>\n",
       "      <td>449</td>\n",
       "      <td>1</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44332</th>\n",
       "      <td>35</td>\n",
       "      <td>unemployed</td>\n",
       "      <td>single</td>\n",
       "      <td>tertiary</td>\n",
       "      <td>cellular</td>\n",
       "      <td>29</td>\n",
       "      <td>jul</td>\n",
       "      <td>200</td>\n",
       "      <td>4</td>\n",
       "      <td>failure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4432</th>\n",
       "      <td>38</td>\n",
       "      <td>self-employed</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>unknown</td>\n",
       "      <td>20</td>\n",
       "      <td>may</td>\n",
       "      <td>775</td>\n",
       "      <td>1</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38725</th>\n",
       "      <td>35</td>\n",
       "      <td>blue-collar</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>cellular</td>\n",
       "      <td>15</td>\n",
       "      <td>may</td>\n",
       "      <td>1313</td>\n",
       "      <td>7</td>\n",
       "      <td>failure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38581</th>\n",
       "      <td>44</td>\n",
       "      <td>services</td>\n",
       "      <td>single</td>\n",
       "      <td>secondary</td>\n",
       "      <td>cellular</td>\n",
       "      <td>15</td>\n",
       "      <td>may</td>\n",
       "      <td>550</td>\n",
       "      <td>2</td>\n",
       "      <td>failure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15913</th>\n",
       "      <td>40</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>cellular</td>\n",
       "      <td>22</td>\n",
       "      <td>jul</td>\n",
       "      <td>87</td>\n",
       "      <td>3</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20622</th>\n",
       "      <td>38</td>\n",
       "      <td>management</td>\n",
       "      <td>married</td>\n",
       "      <td>tertiary</td>\n",
       "      <td>cellular</td>\n",
       "      <td>12</td>\n",
       "      <td>aug</td>\n",
       "      <td>1092</td>\n",
       "      <td>5</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41684</th>\n",
       "      <td>50</td>\n",
       "      <td>blue-collar</td>\n",
       "      <td>married</td>\n",
       "      <td>primary</td>\n",
       "      <td>telephone</td>\n",
       "      <td>2</td>\n",
       "      <td>oct</td>\n",
       "      <td>382</td>\n",
       "      <td>2</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32749</th>\n",
       "      <td>38</td>\n",
       "      <td>technician</td>\n",
       "      <td>single</td>\n",
       "      <td>tertiary</td>\n",
       "      <td>cellular</td>\n",
       "      <td>17</td>\n",
       "      <td>apr</td>\n",
       "      <td>222</td>\n",
       "      <td>1</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37155</th>\n",
       "      <td>53</td>\n",
       "      <td>admin.</td>\n",
       "      <td>single</td>\n",
       "      <td>secondary</td>\n",
       "      <td>cellular</td>\n",
       "      <td>13</td>\n",
       "      <td>may</td>\n",
       "      <td>101</td>\n",
       "      <td>1</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13564 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       age            job   marital education_qual  call_type  day  mon   dur  \\\n",
       "37735   40    blue-collar  divorced      secondary   cellular   14  may   449   \n",
       "44332   35     unemployed    single       tertiary   cellular   29  jul   200   \n",
       "4432    38  self-employed   married      secondary    unknown   20  may   775   \n",
       "38725   35    blue-collar   married      secondary   cellular   15  may  1313   \n",
       "38581   44       services    single      secondary   cellular   15  may   550   \n",
       "...    ...            ...       ...            ...        ...  ...  ...   ...   \n",
       "15913   40       services   married      secondary   cellular   22  jul    87   \n",
       "20622   38     management   married       tertiary   cellular   12  aug  1092   \n",
       "41684   50    blue-collar   married        primary  telephone    2  oct   382   \n",
       "32749   38     technician    single       tertiary   cellular   17  apr   222   \n",
       "37155   53         admin.    single      secondary   cellular   13  may   101   \n",
       "\n",
       "       num_calls prev_outcome  \n",
       "37735          1      unknown  \n",
       "44332          4      failure  \n",
       "4432           1      unknown  \n",
       "38725          7      failure  \n",
       "38581          2      failure  \n",
       "...          ...          ...  \n",
       "15913          3      unknown  \n",
       "20622          5      unknown  \n",
       "41684          2      unknown  \n",
       "32749          1      unknown  \n",
       "37155          1      unknown  \n",
       "\n",
       "[13564 rows x 10 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    45211.000000\n",
       "mean       258.163080\n",
       "std        257.527812\n",
       "min          0.000000\n",
       "25%        103.000000\n",
       "50%        180.000000\n",
       "75%        319.000000\n",
       "max       4918.000000\n",
       "Name: dur, dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dur.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.iloc[41684]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0\n",
       "1        0\n",
       "2        0\n",
       "3        0\n",
       "4        0\n",
       "        ..\n",
       "45206    1\n",
       "45207    1\n",
       "45208    1\n",
       "45209    0\n",
       "45210    0\n",
       "Length: 45211, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
